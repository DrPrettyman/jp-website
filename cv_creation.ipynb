{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blink SEO Work Details\\n\\n\\nI started at Blink as the only technical team member at a small SEO agency with the brief \"use data to improve our processes\".\\nI quickly realised that the SEO team was spending most of their time on data engineering by hand:\\npulling data from various sources (Google Analytics, Shopify, etc., and site crawls using ScreamingFrog), then cleaning it and analysing it all in spreadsheets.\\n\\nI created a python backend wrapping several web APIs to automate the data engineering process,\\nincluding the PyGoogalytics library to pull data from Google Analytics, Search Console and Google Ads.\\nThese data are cleaned and streamed to a Big Query database with a schema I designed to standardise the data and\\nseveral views and procedures to make it easier to query.\\nI also created a (*PostgreSQL*-based) job-queue system to manage asynchronous tasks, ensuring that data imports,\\nanalysis, and ML-assisted recommendations could be processed efficiently, and a user-friendly front-end\\nin Looker Studio which I later migrated to Retool.\\n\\n![Flow diagram showing manual data engineering pasting csv exports into Excel](images/data_diagram.png)\\n\\n![Flow diagram showing automated data engineering](images/data_diagram2.png)\\n\\nI then began adding ML features into the backend to provide actionable insights to the SEO team.\\nThis meant frequent consultation with the team to understand their processes and needs with continual feedback and deployment.\\n\\nThe system was successful in speeding up the SEO process by over 20× and the management team decided to\\nmarket it as a SaaS product to other SEO agencies and, eventually, ecommerce businesses: Macaroni Software.\\n\\nKey achievements:\\n-----------------\\n\\n* I delivered Macaroni Software, allowing SEO teams to deliver\\n  [a year\\'s-worth of work in a single month](https://www.linkedin.com/posts/sam-wright-17b6ab6_shopify-seo-activity-7170336529146441729-JGDn?utm_source=combined_share_message&utm_medium=member_desktop).\\n* I wrote the [PyGoogalytics](https://pypi.org/project/pygoogalytics/) open-source library to manage data ingestion from Google Analytics,\\n  Search Console and Google Ads, wrapping several API calls in one library and standardising the data\\n  (see [mention on LinkedIn](https://www.linkedin.com/posts/sam-wright-17b6ab6_github-blink-seopygoogalytics-activity-7234957676437274624-ipnf/)).\\n* I implemented several ML features to provide actionable recommendations for the SEO team, including:\\n  + Keyword clustering using *ScikitLearn* and *NLTK*, combined with\\n    quantitative data (clicks, impressions, etc.), to identify content gaps and opportunities.\\n  + LLM integration (*huggingface*) to generate ready-to-go suggestions for new content.\\n  + Automated site taxonomy suggestions using clustering and classification algorithms (*ScikitLearn*).\\n* Quantitative data are used for various visualisations in the app, powered by *Plotly*.\\n* Client onboarding, data imports, data analysis tasks and ML-assisted recommendations can be triggered\\n  in the app and run asynchronously in the backend using a (*PostgreSQL*-based) job-queue system which I wrote for this purpose.\\n\\nOther Responsibilities:\\n-----------------------\\n\\n* Presenting to the team and stakeholders on the progress of Macaroni and the insights it has generated.\\n* Besides conceiving of and working on Macaroni, I also did frequent,\\n  one-off data tasks for the SEO delivery and management teams including\\n  data engineering, content generation, and providing analyses and\\n  visualisations for clients and investors.\\n\\nTechnologies:\\n-------------\\n\\nPython, SQL, Pandas, Machine Learning, NLP, ScikitLearn, NLTK, Huggingface, Plotly,\\nWeb Scraping,\\nLooker, Retool, JavaScript, Unix, Linux, Bash,\\nCGP Products, Compute Engine, Big Query, PostgreSQL, Docker, Git,\\nGoogle Analytics, Google Search Console, Google Ads, Shopify API.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from markdownify import markdownify as md\n",
    "\n",
    "def convert_html_to_markdown(html_file_path, markdown_file_path):\n",
    "    # Read HTML file\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Convert HTML to Markdown\n",
    "    markdown_content = md(html_content)\n",
    "    \n",
    "    # Write Markdown to file\n",
    "    with open(markdown_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(markdown_content)\n",
    "    \n",
    "    return markdown_content\n",
    "\n",
    "# Example usage\n",
    "convert_html_to_markdown('src/assets/cv-professional/blink.html', 'blink.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "with open(os.path.join(cwd, \"src\", \"assets\", \"cv-professional\", \"blink.html\"), \"r\") as file:\n",
    "\tsoup = BeautifulSoup(file, \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I created a python backend wrapping several web APIs to automate the data engineering process, including the PyGoogalytics library to pull data from Google Analytics, Search Console and Google Ads. These data are cleaned and streamed to a Big Query database with a schema I designed to standardise the data and several views and procedures to make it easier to query. I also created a (PostgreSQL-based) job-queue system to manage asynchronous tasks, ensuring that data imports, analysis, and ML-assisted recommendations could be processed efficiently, and a user-friendly front-end in Looker Studio which I later migrated to Retool.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = soup.find_all([\"p\", \"ul\"])\n",
    "re.sub(r'\\s+', ' ', tags[1].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>\n",
       "             I started at Blink as the only technical team member at a small SEO agency with the brief \"use data to improve our processes\". \n",
       "             I quickly realised that the SEO team was spending most of their time on data engineering by hand: \n",
       "             pulling data from various sources (Google Analytics, Shopify, etc., and site crawls using ScreamingFrog), then cleaning it and analysing it all in spreadsheets.\n",
       "         </p>,\n",
       " <p>\n",
       "             I created a python backend wrapping several web APIs to automate the data engineering process, \n",
       "             including the PyGoogalytics library to pull data from Google Analytics, Search Console and Google Ads.\n",
       "             These data are cleaned and streamed to a Big Query database with a schema I designed to standardise the data and \n",
       "             several views and procedures to make it easier to query.\n",
       "             I also created a (<em>PostgreSQL</em>-based) job-queue system to manage asynchronous tasks, ensuring that data imports, \n",
       "             analysis, and ML-assisted recommendations could be processed efficiently, and a user-friendly front-end\n",
       "             in Looker Studio which I later migrated to Retool.\n",
       "         </p>,\n",
       " <p>\n",
       "             I then began adding ML features into the backend to provide actionable insights to the SEO team. \n",
       "             This meant frequent consultation with the team to understand their processes and needs with continual feedback and deployment.\n",
       "         </p>,\n",
       " <p>\n",
       "             The system was successful in speeding up the SEO process by over 20× and the management team decided to \n",
       "             market it as a SaaS product to other SEO agencies and, eventually, ecommerce businesses: Macaroni Software.\n",
       "         </p>,\n",
       " <ul>\n",
       " <li>I delivered Macaroni Software, allowing SEO teams to deliver \n",
       "             <a href=\"https://www.linkedin.com/posts/sam-wright-17b6ab6_shopify-seo-activity-7170336529146441729-JGDn?utm_source=combined_share_message&amp;utm_medium=member_desktop\">a year's-worth of work in a single month</a>.</li>\n",
       " <li>I wrote the <a href=\"https://pypi.org/project/pygoogalytics/\">PyGoogalytics</a> open-source library to manage data ingestion from Google Analytics,\n",
       "             Search Console and Google Ads, wrapping several API calls in one library and standardising the data\n",
       "             (see <a href=\"https://www.linkedin.com/posts/sam-wright-17b6ab6_github-blink-seopygoogalytics-activity-7234957676437274624-ipnf/\">mention on LinkedIn</a>).</li>\n",
       " <li>I implemented several ML features to provide actionable recommendations for the SEO team, including:\n",
       "             <ul>\n",
       " <li>Keyword clustering using <em>ScikitLearn</em> and <em>NLTK</em>, combined with \n",
       "                     quantitative data (clicks, impressions, etc.), to identify content gaps and opportunities.</li>\n",
       " <li>LLM integration (<em>huggingface</em>) to generate ready-to-go suggestions for new content.</li>\n",
       " <li>Automated site taxonomy suggestions using clustering and classification algorithms (<em>ScikitLearn</em>).</li>\n",
       " </ul>\n",
       " </li>\n",
       " <li>Quantitative data are used for various visualisations in the app, powered by <em>Plotly</em>.</li>\n",
       " <li>Client onboarding, data imports, data analysis tasks and ML-assisted recommendations can be triggered \n",
       "             in the app and run asynchronously in the backend using a (<em>PostgreSQL</em>-based) job-queue system which I wrote for this purpose.</li>\n",
       " </ul>,\n",
       " <ul>\n",
       " <li>Keyword clustering using <em>ScikitLearn</em> and <em>NLTK</em>, combined with \n",
       "                     quantitative data (clicks, impressions, etc.), to identify content gaps and opportunities.</li>\n",
       " <li>LLM integration (<em>huggingface</em>) to generate ready-to-go suggestions for new content.</li>\n",
       " <li>Automated site taxonomy suggestions using clustering and classification algorithms (<em>ScikitLearn</em>).</li>\n",
       " </ul>,\n",
       " <ul>\n",
       " <li>Presenting to the team and stakeholders on the progress of Macaroni and the insights it has generated.</li>\n",
       " <li>Besides conceiving of and working on Macaroni, I also did frequent, \n",
       "             one-off data tasks for the SEO delivery and management teams including \n",
       "             data engineering, content generation, and providing analyses and \n",
       "             visualisations for clients and investors.</li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(element):\n",
    "\twhile True:\n",
    "\t\ta = element.find(\"a\")\n",
    "\t\tif a is None:\n",
    "\t\t\tbreak\t\n",
    "\t\ts = a.text + \" [link:\" + a.get(\"href\") + \"]\"\n",
    "\t\ta.replace_with(s)\n",
    "  \n",
    "def strip_text(text):\n",
    "\treturn re.sub(r'\\s+', ' ', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "for t in tags:\n",
    "\tparse_links(t)\n",
    "\telements.append(strip_text(t.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I started at Blink as the only technical team member at a small SEO agency with the brief \"use data to improve our processes\". I quickly realised that the SEO team was spending most of their time on data engineering by hand: pulling data from various sources (Google Analytics, Shopify, etc., and site crawls using ScreamingFrog), then cleaning it and analysing it all in spreadsheets.',\n",
       " 'I created a python backend wrapping several web APIs to automate the data engineering process, including the PyGoogalytics library to pull data from Google Analytics, Search Console and Google Ads. These data are cleaned and streamed to a Big Query database with a schema I designed to standardise the data and several views and procedures to make it easier to query. I also created a (PostgreSQL-based) job-queue system to manage asynchronous tasks, ensuring that data imports, analysis, and ML-assisted recommendations could be processed efficiently, and a user-friendly front-end in Looker Studio which I later migrated to Retool.',\n",
       " 'I then began adding ML features into the backend to provide actionable insights to the SEO team. This meant frequent consultation with the team to understand their processes and needs with continual feedback and deployment.',\n",
       " 'The system was successful in speeding up the SEO process by over 20× and the management team decided to market it as a SaaS product to other SEO agencies and, eventually, ecommerce businesses: Macaroni Software.',\n",
       " \"I delivered Macaroni Software, allowing SEO teams to deliver a year's-worth of work in a single month [link:https://www.linkedin.com/posts/sam-wright-17b6ab6_shopify-seo-activity-7170336529146441729-JGDn?utm_source=combined_share_message&utm_medium=member_desktop]. I wrote the PyGoogalytics [link:https://pypi.org/project/pygoogalytics/] open-source library to manage data ingestion from Google Analytics, Search Console and Google Ads, wrapping several API calls in one library and standardising the data (see mention on LinkedIn [link:https://www.linkedin.com/posts/sam-wright-17b6ab6_github-blink-seopygoogalytics-activity-7234957676437274624-ipnf/]). I implemented several ML features to provide actionable recommendations for the SEO team, including: Keyword clustering using ScikitLearn and NLTK, combined with quantitative data (clicks, impressions, etc.), to identify content gaps and opportunities. LLM integration (huggingface) to generate ready-to-go suggestions for new content. Automated site taxonomy suggestions using clustering and classification algorithms (ScikitLearn). Quantitative data are used for various visualisations in the app, powered by Plotly. Client onboarding, data imports, data analysis tasks and ML-assisted recommendations can be triggered in the app and run asynchronously in the backend using a (PostgreSQL-based) job-queue system which I wrote for this purpose.\",\n",
       " 'Keyword clustering using ScikitLearn and NLTK, combined with quantitative data (clicks, impressions, etc.), to identify content gaps and opportunities. LLM integration (huggingface) to generate ready-to-go suggestions for new content. Automated site taxonomy suggestions using clustering and classification algorithms (ScikitLearn).',\n",
       " 'Presenting to the team and stakeholders on the progress of Macaroni and the insights it has generated. Besides conceiving of and working on Macaroni, I also did frequent, one-off data tasks for the SEO delivery and management teams including data engineering, content generation, and providing analyses and visualisations for clients and investors.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            I created a python backend wrapping several web APIs to automate the data engineering process, \\n            including the PyGoogalytics library to pull data from Google Analytics, Search Console and Google Ads.\\n            These data are cleaned and streamed to a Big Query database with a schema I designed to standardise the data and \\n            several views and procedures to make it easier to query.\\n            I also created a (PostgreSQL-based) job-queue system to manage asynchronous tasks, ensuring that data imports, \\n            analysis, and ML-assisted recommendations could be processed efficiently, and a user-friendly front-end\\n            in Looker Studio which I later migrated to Retool.\\n        '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_text(text):\n",
    "\treturn re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "cwd = os.getcwd()\n",
    "with open(os.path.join(cwd, \"src\", \"assets\", \"publications\", \"manifest.json\"), \"r\") as file:\n",
    "\tpubs = json.load(file)\n",
    " \n",
    "for p in pubs:\n",
    "    with open(os.path.join(cwd, \"src\", \"assets\", \"publications\", p[\"entryId\"] + \".txt\"), \"r\") as file:\n",
    "        text = file.read()\n",
    "    texts = [strip_text(t) for t in text.split(\"\\n\\n\")]\n",
    "    p[\"abstract\"] = \"\\n\\n\".join(texts)\n",
    "    \n",
    "with open(os.path.join(cwd, \"src\", \"assets\", \"publications\", \"manifest.json\"), \"w\") as file:\n",
    "    json.dump(pubs, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many dynamical systems experience sudden shifts in behaviour known as tipping points or critical transitions, often preceded by the 'critical slowing down' (CSD) phenomenon whereby the recovery times of a system increase as the tipping point is approached. Many attempts have been made to find a tipping point indicator: a proxy for CSD, such that a change in the indicator acts as an early warning signal.\n",
      "\n",
      "Several generic tipping point indicators have been suggested, these include the power spectrum (PS) scaling exponent whose use as an indicator has previously been justified by its relationship to the well-established detrended fluctuation analysis (DFA) exponent. In this paper we justify the use of the PS indicator analytically, by considering a mathematical formulation of the CSD phenomenon.\n",
      "\n",
      "We assess the usefulness of estimating the PS scaling exponent in a tipping point context when the PS does not exhibit power-law scaling, or changes over time. In addition we show that this method is robust against trends and oscillations in the time series, making it a good candidate for studying resilience of systems with periodic oscillations which are observed in ecology and geophysics.\n"
     ]
    }
   ],
   "source": [
    "print(pubs[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
